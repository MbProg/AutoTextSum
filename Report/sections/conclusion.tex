\subsection{Conclusion}
We tried two different approaches to handle the problem of nugget selection, and used the resulting nuggets to generate summaries of the original documents.
Our first approach worked wordwise by feeding averaged word embeddings of the context in a classifier.\\
The sequence classification approach on the other hand classifies all possible sequences in any sentence as nuggets or not. Sentence embeddings as well as word embeddings were used as features. 
The second approach was unable to learn from the data and always predicted no nugget even though we included oversampling of the minority class. We think this might be because in this approach two very similar nuggets, with only a difference of one word, can have completely opposite labels. Meanwhile a completely uninformative nugget candidate has the same label and loss as the almost correct nugget. This makes the task very hard to learn, especially given that we did not have that much training data. That is probably why the first approach worked a somewhat better in the end. Surprisingly it even managed to aid summaries of group 5's system that were better or on par with the given baselines despite relatively unsatisfying nugget evaluation results. However most other groups' outperformed our results which might have multiple possible explanations. First we suspect that incompatibilities between the nugget selection and group 5's pipeline played an important role in the underwhelming overall performance as their performance might have been finetuned to their nugget selection. Secondly, more labeled annotations, a wider selection of topics and a non sequential approach could also have improved our results by a good decent margin.