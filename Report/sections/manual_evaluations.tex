The summaries are given to human annotators for evaluation. The annotators are students who attend the same course but are in another work group (?). For evaluation Likert Scales are used. Since reference summaries do not exist it can't be evaluated by comparing a summary with a gold standard. Furthermore the annotators shouldn't have to read all ... source documents of a summary to judge the summary itself. This process woud be too time-consuming. Instead items are used on the Likert Scale which can be judged by only reading the summary itself. In total there are eleven categories: "Grammaticality", Non-redundancy", Referential clarity", "Focus", "Structure", "Coherence", "Readability", "Information Content", "Spelling", "Length" and "Overall Quality". For each category the annotators should assign a score from 1 (= very poor) to 5 (= very good), a weight and a confidence (both scales also from 1 to 5) of their grading. For each category the annotators are also free to give a comment to explain their rating. Each summary is evaluated by four to five different annotators.

Besides the summaries of all groups summaries created by two simple approaches (footnote) are evaluated as well. These summaries serve as baseline summaries. The first approach is ... The second approach is ... 


Most categories seem like any text evaluation categories like "Spelling" and "Grammaticality". Other categories seem especially summary-related. These are the categories "Information Content" and "Focus". They represent the goal of a summary very well which is to present the most important content of the summarized texts. Since all summarized texts in this corpus are about a certain query the focus should be visible, too.

The resulting evaluations can be used for assessing the quality of the summaries produced by our system. It is important for the evaluation that we only work at the nugget extraction. This input is given to another group which then produced the summaries. In this way we are completely responsible for the results in some evaluation categories while other evaluation results also depend on the steps of  building the hierarchy and actually creating a summary. The output which we after the nugget extraction are whole sentences (more about the output in section ...). The summary is then only built out of these sentences. In this way all categories which just operate on a sentence level are completely our responsibility. Among these categories are strictly only the two categories "Spelling" and "Grammaticality". We are also highly responsible for the categories "Information Content", "Focus" and "Non-Redundancy". All extracted sentences should ideally contain importannt information related to the query. Furthermore it can be argued that in the step of nugget extraction nuggets with the same meaning as another nugget are ignored. The categories "Referential Clarity", "Structure" and "Coherence" in comparison are very dependent on the ordering of the sentences. It can be argued that "Referential clarity" is also influenced by the nugget extraction. For sentences with a pronoun the system should also extract the reference sentence. Otherwise the sentence is not well usable in the next steps. This is not done in the step of nugget extraction, but in later steps. The category "Length" especially depends on the last step, the summary creation. "Readability" and of course "Information Content" are very general categories which can't be assigned to any particular step. The focus of our analysis will be all steps which can be influenced by our work, the nugget extraction. Thus the categories "Structure", "Length" and "Coherence" will only be shortly discussed.

In the following we compare the results of our group with the results of the other groups and the two baseline approaches. Our average overall score is 2.86. The average overall scores of the other groups are 0.39 to 0.74 points better. In contrast to the baseline approaches our summaries are much better. The baseline approaches only have an average overall score of 1.61 and 1.62. So our approach is more than one point better than the baselines. Now we take a closer look at the different categories. "Overall Qualiy" isn't discussec here because it does not highlight a particular aspect of a summary. Compared to the other groups our summaries are worst in all categories except for "Referential Clarity". In the category "Information Content" which is very important for summaries we outdo both baseline approaches significantly at least. The categories we are best at are "Spelling" with ..., "Non-Redundancy" with ... and "Grammaticality" with .... The other groups also perform best at "Grammaticality" and "Spelling"??? This is not surprising since all groups extracted whole sentences for the summarization. These sentences should be mostly grammatical, correctly spelled sentences. Perhaps there are some exceptions since the sentences are taken from forum posts. categories we are worst in are "Structure" with 2.86 points, "Coherence" with 2.88 points and "Information Content" with 2.9 points. "Structure and "Coherence" are also the categories the other groups perfom worst at.

Since we use only full sentences for the creation of the summaries it is surprising that the results uin "Grammaticality" and "Spelling" are not near the maximum score. The comments of the annotators hint at certain repeatedly made mistakes. Many of them are related to the fact that the source texts are taken from forum posts which can contain mistakes like this. Some sentences contain punctuation error like missing dots or quotes. Annotators critisize incomplete sentences like "The study of mechanical self propulsion in vehicles." which often seem like headlines.  There are also summaries which consist of only one long sentence like "Developing performance-enhancing behavioral therapies for individuals prenatally exposed to alcohol and focusing remediation efforts on disabilities that affect quality of life and everyday functioning Information about illicit drugs, alcohol, prevention and treatment programs can be obtained on the following websites: Being raised in a family where abuse of alcohol or other substances (illegal drugs or prescription medications) occurs can lead to a host of challenges for children." All these problems can be solved in different ways. A possible solution for punctuation errors is to check if a sentence ends with a punctuation sign and to check if parentheses and quotes are properly closed. For the removal of incomplete sentences a POS tagger can be used. It should check if a sentence contains at least a noun and a verb. Extremely long sentences can be just filtered out with a certain threshold length. In this way also too short sentences which can also cause problems can be filtered out.

Now we take a look at differnt errors in rhe category "Spelling". This category contains some punctuation errors, too. It seems like annotators do not know in which category these kinds of errors belong. In this case the annotation protocol needs to be specified. A mistake unique to the category "Spelling" is incorrect upper- and lowercasing. Another mistake is wrong whitespacing, like in "loans , you". The upper- and lowercasing could be handled by a POS tagger so that only proper names are uppercased and everything else is lowercased. Additional whitespaces can be easily removed with a regular expression.

As we see the categories "Grammaticality" and "Spelling" contain many mistakes which can be fixed quite easily. That means that actual improvement in these categories can be achieved well.

Now we will take a look at the categories "Information Content", "Focus" and "Non-Redundancy". "Information Content" is one of our system's greatest weaknesses. Annotators' comments point towards the relatedness of "Information Content" and "Non-Redundancy", "Focus" and "Readability". If a text contains only one fact over and over, if it contains facts unrelated to the topic or if it is not understandable there is no real information gain. So it is very important to optimize the results in these categories to impart as many information as possible. The score in focus of 3.14 is much better than of baseline 1 (2.15) but slightly worse than the score in "Focus" of baseline 2. We integrate the query in our nugget extraction by averaging the query with a nugget. It seems like we need addditional features to incorporate the query. This can be focus of future work. The results of our system of our system in "Non-Redundancy" are worse than the ones of baseline 1 but similar to the results of group 5 and baseline 2. The similarity to group 5 is very interesting since we used the pipeline after the nugget extraction from this group. It hints that group 5's system does not properly remove duplicates while creating a summary. An extreme example is the following summary which consists of four sentences with a content nearly identical: "Computer Explorers uses innovative and creative ways to excite young learners about science, technology, engineering and math subjects. The local Computer Explorers uses technology in creative ways to engage students in science, math, English and other core academic subjects. Computer Explorers is an education company that uses technology in creative ways to engage students in science, math, English and other core academic subjects. Computer Explorers is a local education company that uses technology in innovative ways to engage students in science, math, English and other core academic subjects". It sees like no similarity detection is used. This does not nessecarily have to be done in summary creation but can be also done in the nugget extraction, at least if full sentences are extracted.

