\subsection{Future Work}
Due to lack of time and computational resources we could only try out a  limited amount of architectures and models.
Our first approach might have worked better with a more sophisticated technique to combine word embeddings. A simple weighted average by word frequency like proposed recently may already bring significant improvements \cite{sif2016}.\\
We think the second approach was promising, but perhaps it would need to be computed more efficiently to make it feasible. Regression given a word sequence as input seems like a very little researched problem. This could be a promising research direction, in order to extend the success of RNNs into new areas. Perhaps for this problem the label space would have to be more aligned to the embedding space. For example by including a notion of similarity to the true nuggets. When the model predicts a nugget which no worker annotated but it is only missing a single word for the true nugget, the model needs to have lower loss to notice the improvement over a completely different sequence.
A third approach we would have tried with a bit more time on our hands is sequence to sequence classification. The state of the art results with RNNs are incredible, but the research on smaller datasets is limited. It would be interesting to see if these techniques would work on this problem.\\
Another area of improvement would be the rest of the pipeline of course. Sadly our approach did not fit that well to the group we partnered with for summary generation, since they assumed nuggets to be whole sentences. Designing a whole hierarchical summarization pipeline based on nugget phrases, not sentences, has many unique requirements as well as opportunities for innovative research.