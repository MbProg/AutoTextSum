We use a heterogeneous multi-document summarization corpus called \textit{hMDS} by Zopf, Peyrard and Eckle-Kohler (2016) \citep{tubiblio97941} as an additional training resource. It is publicly available and in its broad range of text type it is similar to our main corpus.

The corpus is created in a novel, reverse way with the help of human annotators: First summaries are collected and then source documents for the summaries are retrieved. As summaries the first part of Wikipedia articles about a certain topic is taken. Then human annotators mark 10 to 20 atomic information nuggets in these summaries. The next step is retrieving for each information nugget a text on the web which contains the nugget verbatim. The type of the texts is chosen in such a way that as many different text types as possible are collected for each topic. Among the different text types are forum posts, articles, interviews and scientific articles. In result the corpus consists of 91 summaries about topics in the fields of history, politics and art with 1265 source documents.

We only extract sentences out of the corpus' source documents which at least contain one nugget. In this way we create more positive training examples since a problem while training was a very high amount of negative training instances.