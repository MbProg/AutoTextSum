\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

\usepackage{subfiles}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[]{algorithm2e}
\usepackage{graphicx}

\title{Automatic text summarization, 2018}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Today there are many documents, articles, papers and reports available in digital form. These volumes of text are invaluable sources of information and knowledge that need to be effectively summarized to be useful. In automatic text summarization machine learning techniques are often used to generate summaries. A prior step to the generation of summaries is usually the extraction of nuggets. This paper presents the two approaches we use for the extraction of nuggets, as well as a description of their effectiveness and shortcomings. 
  
\end{abstract}

\section{Introduction}
With the dramatic growth of the web, people are overwhelmed by the tremendous amount of online information and documents. This expansion in availability of data has demanded extensive research in the automatic generation of summaries from a collection of different type of text.\\

\textit{Automatic summarization} is the process of shortening a text document with software, in order to create a summary with the major points of the original document.	     

In general, there  are two different approaches for text summarization:\textit{extraction} and \textit{abstraction}


\section{Implementation}
\label{headings}


For all our Implementations we chose two build flexible models which would not only be able to choose whole Sentences as nuggets but also sub parts. Justification for that was drawn from short inspection of the nuggets that workers had chosen. For this we first calculated the percentage of nuggets that were sentences by counting nuggets that start with uppercasing and end on a punctuation mark and dividing it by the total nugget amount. Additionally we also plotted a histogram of nugget lengths that can be seen in figure
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.55\linewidth]{Nugget_size.pdf}
	\caption{Nugget length distribution}
	\label{fig:nuggets}
\end{figure}
\ref{fig:nuggets}.
The results were that only about $49$ percent of the chosen nuggets are actual sentences and most of the nuggets are shorter than $15$ words. We then came up with two main approaches on how to deal with that flexibility.

\subsection{First approach}

For the first approach we chose to take a wordwise approach that was also inspired by findings additional to those mentioned above. This finding is that workers may choose different start or endpoints for nuggets leading to problems for approaches that would only consider nuggets important that match exactly for multiple workers. An example in our dataset for this can be found in table \ref{tab:nuggets}.
\begin{center}
	\begin{tabular}{ | l | p{7cm} |}
		\hline
		Worker ID & Nugget \\ \hline
		87b87beadeaabc197b466e265837af98 &  While some experts admit that neurofeedback has promise, they believe that it should be used only in combination with medication. \\ \hline
		f0a942943de19e11972338c883ad1da2 &  some experts admit that neurofeedback has promise, they believe that it should be used only in combination with medication. \\ \hline

		87954087f1d66c24165db6afc992e136 &  neurofeedback has promise, \\ \hline

		ec7e848297d5f2666f07c0d779ca074d	 &  neurofeedback has promise, they believe that it should be used only in combination with medication. \\
		\hline
	\end{tabular}
\label{tab:nuggets}
\end{center}
To avoid loss of such parts of sentences that are contained in multiple nuggets we therefore view each word of a sentence as a training / prediction instance. We do this by taking a word window with size $1$ around the particular word of interest which therefore contains the word and its left and right immediate neighbor. All those words are then transformed into word vectors from embeddings pretrained by Google \footnote{https://code.google.com/archive/p/word2vec/} and averaged. Similarily we also transform all word of the query of a specific document into word embeddings and average them. In a third step both averages are averaged together once again to derive our word feature representation. We chose this final averaging step because we thought the arithmetic properties of word embeddings would retain information about the relation between word window and query. This also keeps the feature representation smaller than doing a concatenation for an example and therefore allowed us to still use classifiers that can not train on mini batches.

As labels each word gets assigned a number which represents the amount of nuggets in the same sentence that contain that specific word. The different amounts of workers that chose a nugget then form the label of a classification task. A summarization of the process from a word of a sentence to feature representation can be found in pseudocode in  \ref{alg:features}. Here it is assumed that a sentence is padded accordingly.

\begin{algorithm}
	\label{alg:features}
	\SetKwInOut{Input}{input}

	\SetKwInOut{E}{E}
	\SetKwInOut{Output}{output}
	\SetKwInOut{QueryEmbedding}{QueryEmbedding}
	\SetKwInOut{WindowEmbedding}{WindowEmbedding}
	\SetKwInOut{SentenceWords}{SentenceWords}
	\SetKwInOut{QueryWords}{QueryWords}
	\E{Pre trained word embeddings}
	\QueryEmbedding{Averaged query embedding}
	\WindowEmbedding{Averaged word embedding of the word window}
	\SentenceWords{List of words in the sentence}
	\QueryWords{List of words in the query}

	\For{$i = 0;\ i < length(Query);\ i = i + 1$}{
		QueryEmbedding += Querywords[i]
	}
	QueryEmbedding /= length(Query)\;
	WindowEmbedding = E[SentenceWords[j-1]] + E[SentenceWords[j]] + E[SentenceWords[j+1]]\;
	WindowEmbedding /= 3\;


	return (QueryEmbedding+WindowEmbedding) /2 \;

	\caption{Feature building process}
\end{algorithm}


\begin{algorithm}
	\label{alg:the_alg}
	\SetKwInOut{Input}{input}
	\SetKwInOut{WordScore}{wordscore}
	\SetKwInOut{Classifier}{Classifier}
	\SetKwInOut{Output}{output}
	\SetKwInOut{Delta}{delta}
	\SetKwInOut{TempNugget}{tempNugget}
	\Input{List of words in a sentence}
	\Classifier{The trained word classifier}
	\WordScore{List of tuples containing word and their score}
	\Delta{The nugget word threshold}
	\TempNugget{List of words that form a nugget}
	\KwResult{List of Nuggets}
	\lForEach{word $w$ of input}{ wordscore.append($w$, Classifier(word)) }
	\For{(w, score) in wordscore}{
		\uIf{$score > delta$}{TempNugget.append(w)}
		\uElseIf{$score < delta$ and tempNugget not empty}{
			Result.append(TempNugget) \;
			TempNugget = empty list\;
		}
		\Else{
			skip\;
		}
	}
	return Result \;

	\caption{Nugget prediction process}
\end{algorithm}
To form nuggets in a sentence we first transform each word into the above described feature representation and then assign a class or score with the trained classifier. Afterwards nuggets are formed by concatenating consecutive words that have scores above a pre-defined threshold $\delta$. The whole process is defined in pseudocode in \ref{alg:the_alg}

 \subsection{Second Approach}
For the second approach we used sequence classification to classify each possible candidate as either a nugget or not. More precisely we generated a candidate $s_{i,m}$ for each word $w_i$ with $m$ following words. We iterated the candidate length $m=|s_{i,m}|$ between a minimum length of 5 and a maximum of 20, if the length of the sentence allowed it. So $5<=m<=20$. So in the end we would have every possible candidate in any given sentence. \\
This preprocessing required substantive computational time, so we saved these candidates to the disk to have the whole training set available and speed up the training process. We found that only a few percent of possible nugget candidates actually were annotated by at least one worker. This extreme class imbalance can lead to serious issues in the learning process with models models \citep{japkowicz2002class}. To fix this issue we used oversampling of the rare class, i.e. after shuffling all samples we would take 20\% candidates with a label bigger than zero and 80\% random candidates. This makes it much more likely to have at least one positive sample in each batch, so that the model can have a more stable training signal.

We chose this approach because it seems like a intuitive solution to the problem that nuggets can also be incomplete sentences. In many of the target nuggets we were given, the workers annotated only the most important phrases of a sentence.

To represent the nugget candidates we used sequences of pretrained Word2Vec word embeddings \cite{w2v}, as well as sentence embeddings computed by the Universal Sentence Encoder module in tensorflow \cite{universal2018}.

To identify the correct nuggets we could use classification with two classes: nugget or not a nugget. This makes the task similar to sentence classification, which is a very well researched problem. The difference is that nuggets can be incomplete sentences. As \citep{zhou2015c} and others have shown, RNNs are among the top performing models in sentence classification together with CNNs, if given enough training data. For the target data we used a threshold of two, so any phrase which was annotated as a nugget by two or more workers would be considered a nugget in this binary format. Also possible would be regression of the number of Mechanical Turk workers annotating each word sequence as a nugget. Which would be equivalent to the original format of our target data.\\

We used GRUs as proposed by \citet{gru2014}, because we have less training data than most state of the art research projects. While the performance of LSTMs and GRUs are very similar, the GRU has a bit fewer parameters to train and is thus less computationally demanding \cite{cnn_rnn_comparative2017}.

\subsection{Generating summaries}
Since the ultimate goal of the project was to generate summaries. As suggested by the advisors we chose to collaborate with group $5$. In order to make their pipeline accessible for our solutions they gave us the requirement of providing 30 sentences per topic sorted by their importance for the summary. As our approaches did not necessarily lead to sentence outputs we had to come up with a workaround to fulfill the necessary requirements.  

We eventually decided to proceed the following way. For every sentence of a topic we calculate the percentage of words that appear in nuggets chosen by our model. Sentences that have a percentage smaller than $40$ percent are excluded. Afterwards we gave group $5$ our top $30$ sentences with regards to words in a nugget in descending order and then received their final model output which is further evaluated in the Evaluation section.   
\section{Evaluation}
\subsection{Nugget Evaluation}
We tried to evaluate both our main approaches on the labeled dataset %add corpus ref%
.During training of the second approach we noticed that the neural network used was not able to learn anything other than predicting the majority class. We tried several different network parameters, as well as data sub- and oversampling as well as turning it into a regression task however none of those measures seemed to help to alleviate the problem. This is why we abandoned the approach for the final predictions and also don't report performance measures for that approach. For the evaluation we chose $2$ out of the $10$ labeled topics and split them into one development set topic and a test set topic respectively. As for all the machine learning models we use the popular scikit-learn python module. Since our computational resources were limited we only were able to test three different models in a reasonable timeframe such as Decision trees, logistic Regression and Random forests. For the latter training however took significantly longer than for the other two so less hyperparameters were tested.
The final scores on the Test set can be found in table \ref{Nugget_scores}.
\begin{table}
	\caption{Nugget Evaluation Scores}
	\label{Nugget_scores}
	\centering
	\begin{tabular}{llll}
		\toprule
		\multicolumn{4}{c}{}                   \\

		Model     & Recall & Precision & F1 \\
		\midrule
		DecisionTree & $0.901$  & $0.1501$ &   $0.257$   \\
		RandomForest     & $0.887$ & $0.137$ &   $0.237$   \\
		LogisticRegression     &  $0.891$ & $0.09$ & $0.163$  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Nugget results discussion}
The overall performance of the nugget selection were quite disappointing and can have many different causes. One obvious factor is the relatively unorthodox selection of nuggets which is evident in the high recall scores. This happened mainly because we had to set the threshold of nuggets to $1$ as otherwise no nuggets would have been predicted. We tried to sum class probabilities and modeling the task as regression but the heavy imbalance between label values always led to similar problems in the results. A bigger dataset would have allowed us by the means of subsampling to provide a more balanced class distribution and therefore possibly achieve better results. Also the query or topic should be a rather important information about what is might be a nugget in a sentence. The provided labeled dataset only has ten such topics though which might also not be enough data to allow for a generalizable solution in the first place.  However it may also be concluded that the chosen feature representation due to the averaging or relatively narrow view on the sentence does not provide a clear enough boundary between important and unimportant words/nuggets.

To address the unclear effect of this feature representation we originally thought about using our second approach which however failed as well. We mainly guess that it was due to a lack of annotated data as  it was easy to create vast amount of negative labeled nuggets but the relatively limited amount of queries and annotated nuggets might be problematic for learning a decently sized recurrent neural network.
\subsection{Manual evaluation}

\subfile{sections/manual_evaluations}

\small

\bibliographystyle{plain}
\bibliography{nips_2018}


\end{document}
